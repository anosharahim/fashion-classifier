{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "vgg16.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing SVM and VGG16\n"
      ],
      "metadata": {
        "id": "xx5LSqknUMOl"
      },
      "id": "xx5LSqknUMOl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEPhwNaBUsV_",
        "outputId": "e220ff1b-721f-416a-c77b-8209d6454623",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "CEPhwNaBUsV_",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYshHa96WiIR"
      },
      "source": [
        "#Import necessary packages and libraries\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import svm\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "id": "LYshHa96WiIR",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n"
      ],
      "metadata": {
        "id": "SwelEX7HUIEc"
      },
      "id": "SwelEX7HUIEc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7WOkOIEU-Do",
        "outputId": "e484d7f2-7933-42e4-d9ad-6e30e14391d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "m_files = os.listdir('/content/drive/MyDrive/fashion-data/men')\n",
        "w_files = os.listdir('/content/drive/MyDrive/fashion-data/women')\n",
        "\n",
        "#merge both datasets \n",
        "all_files = m_files + w_files\n",
        "print('total:',len(all_files))"
      ],
      "id": "U7WOkOIEU-Do",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total: 2512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1HcXN4xVW5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1ae9c1-adb6-4eec-b69e-51e744b3b66f"
      },
      "source": [
        "#convert image files into arrays\n",
        "path = '/content/drive/MyDrive/fashion-data'\n",
        "def read_img(file,name,pth):\n",
        "    images = np.zeros((14700))\n",
        "    for image in file: \n",
        "        arr = Image.open(pth+'/'+name+'/'+image) #get img array \n",
        "        img = arr.resize((70,70)) #resize for standard sizing\n",
        "        arr = np.asarray(img) #turn into array\n",
        "        img.close()\n",
        "        flatten = arr.flatten() #flatten\n",
        "        images = np.vstack((images,flatten)) #stack \n",
        "    images = np.delete(images, 0, 0)\n",
        "    return images\n",
        "\n",
        "#read images from files \n",
        "men = read_img(m_files[:500],'men',path)\n",
        "women = read_img(w_files[:500],'women',path)\n",
        "\n",
        "#turn into dataframes and add class labels\n",
        "men = pd.DataFrame(men)\n",
        "men['label'] = 0\n",
        "women = pd.DataFrame(women)\n",
        "women['label'] = 1\n",
        "\n",
        "#merge and shuffle\n",
        "fashion = men.append(women,ignore_index=True)\n",
        "fashion = shuffle(fashion)\n",
        "\n",
        "#separate dependent and independent variables\n",
        "X = fashion.loc[:, fashion.columns != 'label']\n",
        "y = fashion['label']\n",
        "\n",
        "#see dataframe\n",
        "fashion.head()\n",
        "\n",
        "#split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=42)\n",
        "\n",
        "print('done baby')"
      ],
      "id": "v1HcXN4xVW5N",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done baby\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Support Vector Machines\n",
        "- Train a support vector classifier using each of the following kernels:\n",
        "    - Linear\n",
        "    - Poly (degree = 2)\n",
        "    - RBF\n",
        "\n",
        "- If you encounter any issues with training time or memory issues, then you may use a reduced dataset, but carefully detail why and how you reduced the dataset. Unnecessarily reducing the dataset will result in reduced grades!\n",
        "- Report your error rates on the testing dataset for the different kernels.\n"
      ],
      "metadata": {
        "id": "vrIzIrbaT0e-"
      },
      "id": "vrIzIrbaT0e-"
    },
    {
      "cell_type": "code",
      "source": [
        "## LINEAR\n",
        "svm_linear = svm.SVC(kernel='linear')\n",
        "svm_linear.fit(X_train,y_train)\n",
        "print(svm_linear.score(X_test,y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlLddnEBTxp2",
        "outputId": "12f5361d-b9b4-45c6-90f7-776ec4dd5f85"
      },
      "id": "BlLddnEBTxp2",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTddLftvVhPd",
        "outputId": "ec8db85f-dab6-42d6-9999-36713e75008b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## POLY\n",
        "svm_poly = svm.SVC(kernel='poly', degree=2)\n",
        "svm_poly.fit(X_train,y_train)\n",
        "print(svm_poly.score(X_test,y_test))"
      ],
      "id": "ZTddLftvVhPd",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## RBF\n",
        "svm_rbf = svm.SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train,y_train)\n",
        "print(svm_rbf.score(X_test,y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHHu5lL9T5Y0",
        "outputId": "a04d161d-ea05-4a2d-df28-eb7ea8056cc8"
      },
      "id": "pHHu5lL9T5Y0",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Deep Neural Networks\n",
        "Using Keras load the VGG16 network. This is the convolutional neural network which won ImageNet 2014, and the accompanying paper is available here, if you want to read more about it. Keras code to perform this step is available here, under the heading \"Extract features with VGG16.\"\n",
        "\n",
        "- Perform transfer learning using VGG16.\n",
        "- What loss function did you choose, and why?\n",
        "- What performance do you achieve on your test set and how does this compare to the performance you were originally able to achieve with the linear methods?\n",
        "- (optional) If you want, you can also perform a \"fine-tuning\" step. In this step we unfreeze the weights and then perform a few more iterations of gradient descent. This fine tuning can help the network specialize its performance in the particular task that it is needed for. Now, measure the new performance on your test set and compare it to the performance from the previous step."
      ],
      "metadata": {
        "id": "fT-mVMpcT_v9"
      },
      "id": "fT-mVMpcT_v9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df4Liw8ZcWdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784b4abd-3fc2-492d-ce60-5ca3730463ca"
      },
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "\n",
        "### DATA PREPROCESSING ### \n",
        "\n",
        "## TRAINING VGG on one image only\n",
        "\n",
        "#preprocess one image\n",
        "man = path+'/men/'+m_files[1]\n",
        "image = load_img(man, target_size=(224, 224))\n",
        "\n",
        "image = img_to_array(image)\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "image = preprocess_input(image)\n",
        "\n",
        "#LOAD VGG16\n",
        "model = VGG16()\n",
        "\n",
        "# predict the probability across all output classes for\n",
        "yhat = model.predict(image)\n",
        "\n",
        "# convert the probabilities to class labels\n",
        "label = decode_predictions(yhat)\n",
        "# retrieve the most likely result, e.g. highest probability\n",
        "label = label[0][0]\n",
        "# print the classification\n",
        "print('%s (%.2f%%)' % (label[1], label[2]*100))"
      ],
      "id": "Df4Liw8ZcWdm",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shoe_shop (34.26%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MULTIPLE IMAGES \n",
        "\n",
        "#prepaare images in bulk for VGG model\n",
        "def read_img_vgg(files,class_,path_):\n",
        "  imgs = np.zeros((1, 224, 224, 3))\n",
        "  for i in files: \n",
        "    ipath = path_+class_+i\n",
        "    im = load_img(ipath, target_size=(224, 224))\n",
        "    # convert the image pixels to a numpy array\n",
        "    im = img_to_array(im)\n",
        "    # reshape data for the model\n",
        "    im = im.reshape((1, im.shape[0], im.shape[1], im.shape[2]))\n",
        "    # prepare the image for the VGG model\n",
        "    im = preprocess_input(im)\n",
        "    #stack onto the overall list\n",
        "    imgs = np.vstack((imgs,im))\n",
        "  return imgs\n",
        "\n",
        "path_ = '/content/drive/MyDrive/fashion-data'\n",
        "\n",
        "men_ = read_img_vgg(m_files[:500],'/men/',path_)\n",
        "women_ = read_img_vgg(w_files[:500],'/women/',path_)\n"
      ],
      "metadata": {
        "id": "RX3Q8TkEYRo7"
      },
      "id": "RX3Q8TkEYRo7",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison "
      ],
      "metadata": {
        "id": "D_7SUeljUR0R"
      },
      "id": "D_7SUeljUR0R"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UcbLqLdkUTtK"
      },
      "id": "UcbLqLdkUTtK",
      "execution_count": null,
      "outputs": []
    }
  ]
}